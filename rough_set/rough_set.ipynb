{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "79efd5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "b4f02a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fuzzy_similarity(X,weights,sigma=1.0):\n",
    "    # weights are already sigmoid(w)\n",
    "    # X shape: (n,m)\n",
    "\n",
    "    # Weighted difference: (X_i - X_j)^2 * w^2\n",
    "    # We use broadcasting to get (N,M,M) differences\n",
    "    diff=X[:,jnp.newaxis,:]-X[jnp.newaxis,:,:]\n",
    "\n",
    "    weighted_sq_dist=jnp.sum((weights**2)*(diff**2),axis=-1)\n",
    "\n",
    "    return jnp.exp(-weighted_sq_dist/(2 * sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "aba0c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_lower_approximation(R,labels,alpha=10.0):\n",
    "    #labels is the vector of shape (N,) [1,0,1]\n",
    "    #R is the similaity matrix found from the prev func\n",
    "\n",
    "    #Mask[i,j] true if i and j have diff labels\n",
    "    diff_mask=labels[:,jnp.newaxis]!=labels[jnp.newaxis,:]\n",
    "    diff_similarities=jnp.where(diff_mask,R,-1e9)\n",
    "    worst_diff_similarity=jax.nn.logsumexp(alpha*diff_similarities,axis=1)/alpha\n",
    "    mu=1.0-worst_diff_similarity\n",
    "    return mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4c395c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rs_loss(mu):\n",
    "    gamma=jnp.mean(mu)\n",
    "    return 1.0-gamma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "85019bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss_fn(params,X,y,lambda1=0.1,lambda2=0.07):\n",
    "    #converting raw weights into sigmoid so that it lies between 0 and 1\n",
    "    weights=jax.nn.sigmoid(params['w'])\n",
    "\n",
    "    #1. Fuzzy reln\n",
    "    R=compute_fuzzy_similarity(X,weights)\n",
    "    \n",
    "    #2. Soft layer approx\n",
    "    mu=soft_lower_approximation(R,y)\n",
    "\n",
    "    #3. RS loss\n",
    "    l_rs=calculate_rs_loss(mu)\n",
    "\n",
    "    #4. L1 regularisation to improve accuracy (Feature selection penalty)\n",
    "    l_l1=jnp.sum(jnp.abs(weights))\n",
    "\n",
    "    #Total loss= classifier loss (cross entropy)+lambda1*RS_Loss+lambda2\n",
    "    return lambda1*l_rs+lambda2*l_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "68a504ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "key=jax.random.PRNGKey(39)\n",
    "N,M=300,4\n",
    "X=jax.random.normal(key,(N,M))\n",
    "y=(X[:,0]>0).astype(jnp.int32)\n",
    "initial_raw_value = jnp.log(0.6 / (1 - 0.6))\n",
    "#Weight gen\n",
    "params = {\n",
    "    'w': jnp.full((M,), initial_raw_value)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b1b6c917",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(params,x,y):\n",
    "    grads=jax.grad(total_loss_fn)(params,X,y)\n",
    "    #Simple Gradient Descent\n",
    "    return {k : v-0.1*grads[k] for k,v in params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "45e0cc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------TRAINING START--------------------\n",
      "Iter   0 | Loss 0.2625 | Weights: [0.59985614 0.5996786  0.5996767  0.5996796 ]\n",
      "Iter  20 | Loss 0.2613 | Weights: [0.5969839  0.5932225  0.59318274 0.59324396]\n",
      "Iter  40 | Loss 0.2602 | Weights: [0.59412163 0.58671635 0.5866358  0.58675855]\n",
      "Iter  60 | Loss 0.2590 | Weights: [0.59126997 0.5801646  0.58004016 0.5802278 ]\n",
      "Iter  80 | Loss 0.2578 | Weights: [0.5884294  0.57357144 0.57340026 0.5736561 ]\n",
      "Iter 100 | Loss 0.2567 | Weights: [0.5856004  0.5669416  0.56672066 0.567048  ]\n",
      "Iter 120 | Loss 0.2555 | Weights: [0.5827833  0.5602798  0.56000614 0.5604084 ]\n",
      "Iter 140 | Loss 0.2543 | Weights: [0.5799787  0.5535909  0.55326146 0.553742  ]\n",
      "Iter 160 | Loss 0.2531 | Weights: [0.57718706 0.54687977 0.5464916  0.5470539 ]\n",
      "Iter 180 | Loss 0.2520 | Weights: [0.5744086  0.54015136 0.5397016  0.540349  ]\n",
      "Iter 200 | Loss 0.2508 | Weights: [0.5716438  0.5334109  0.53289664 0.5336325 ]\n",
      "Iter 220 | Loss 0.2496 | Weights: [0.5688931  0.52666336 0.5260817  0.52690953]\n",
      "Iter 240 | Loss 0.2485 | Weights: [0.5661566  0.51991385 0.51926214 0.5201852 ]\n",
      "Iter 260 | Loss 0.2473 | Weights: [0.56343484 0.51316756 0.512443   0.5134646 ]\n",
      "Iter 280 | Loss 0.2462 | Weights: [0.5607279  0.5064293  0.5056295  0.50675297]\n",
      "Iter 300 | Loss 0.2450 | Weights: [0.5580362  0.4997043  0.49882665 0.5000552 ]\n",
      "Iter 320 | Loss 0.2439 | Weights: [0.55535984 0.4929973  0.49203953 0.49337626]\n",
      "Iter 340 | Loss 0.2427 | Weights: [0.5526992  0.48631313 0.48527312 0.48672104]\n",
      "Iter 360 | Loss 0.2416 | Weights: [0.5500544  0.4796566  0.4785322  0.48009425]\n",
      "Iter 380 | Loss 0.2405 | Weights: [0.54742557 0.47303218 0.47182155 0.47350058]\n",
      "Iter 400 | Loss 0.2394 | Weights: [0.5448129  0.46644425 0.46514565 0.46694443]\n",
      "Iter 420 | Loss 0.2383 | Weights: [0.5422166  0.4598971  0.45850894 0.46043   ]\n",
      "Iter 440 | Loss 0.2372 | Weights: [0.5396367  0.4533948  0.45191562 0.45396143]\n",
      "Iter 460 | Loss 0.2361 | Weights: [0.5370734  0.44694114 0.44536975 0.4475427 ]\n",
      "Iter 480 | Loss 0.2350 | Weights: [0.53452665 0.44053993 0.4388752  0.44117746]\n",
      "Feature 0: Weight 0.5321 -> SELECTED\n",
      "Feature 1: Weight 0.4345 -> DISCARDED\n",
      "Feature 2: Weight 0.4328 -> DISCARDED\n",
      "Feature 3: Weight 0.4352 -> DISCARDED\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "print(\"----------------------TRAINING START--------------------\")\n",
    "\n",
    "for i in range(500):\n",
    "    params=update(params,X,y)\n",
    "    if i%20==0:\n",
    "        current_weights=jax.nn.sigmoid(params['w'])\n",
    "        loss=total_loss_fn(params,X,y)\n",
    "        print(f\"Iter {i:3} | Loss {loss:.4f} | Weights: {current_weights}\")\n",
    "\n",
    "final_weights = jax.nn.sigmoid(params['w'])\n",
    "for i, w in enumerate(final_weights):\n",
    "    status = \"SELECTED\" if w > 0.5 else \"DISCARDED\"\n",
    "    print(f\"Feature {i}: Weight {w:.4f} -> {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651d4a50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
